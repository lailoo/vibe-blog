"""
çŸ¥è¯†æœåŠ¡ - ç®¡ç†å’Œèåˆå¤šæ¥æºçŸ¥è¯†

ä¸€æœŸç®€åŒ–ç­–ç•¥ï¼š
- æ•´ä¸ªæ–‡æ¡£ä½œä¸º 1 æ¡çŸ¥è¯†ï¼Œä¸åˆ†å—
- åŸºäºæ ‡é¢˜/æ–‡ä»¶åå»é‡
- æ–‡æ¡£çŸ¥è¯†ä¼˜å…ˆäºç½‘ç»œæœç´¢

äºŒæœŸå¢å¼ºï¼š
- æ”¯æŒçŸ¥è¯†åˆ†å—
- ä¸¤çº§ç»“æ„ï¼šæ–‡æ¡£æ‘˜è¦ + åˆ†å—å†…å®¹
- å›¾ç‰‡æ‘˜è¦æ•´åˆ
"""
import os
import re
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Literal

logger = logging.getLogger(__name__)


@dataclass
class KnowledgeItem:
    """çŸ¥è¯†æ¡ç›®ï¼ˆä¸€æœŸç®€åŒ–ç‰ˆï¼‰"""
    source_type: Literal['document', 'web_search']  # æ¥æºç±»å‹
    title: str                                       # æ ‡é¢˜
    content: str                                     # å†…å®¹ï¼ˆä¸€æœŸï¼šæ•´ä¸ªæ–‡æ¡£å†…å®¹ï¼‰
    url: Optional[str] = None                        # ç½‘ç»œæ¥æº URL
    file_name: Optional[str] = None                  # æ–‡æ¡£æ–‡ä»¶å
    relevance_score: float = 0.0                     # ç›¸å…³æ€§è¯„åˆ†
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'source_type': self.source_type,
            'title': self.title,
            'content': self.content,
            'url': self.url,
            'file_name': self.file_name,
            'relevance_score': self.relevance_score
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'KnowledgeItem':
        """ä»å­—å…¸åˆ›å»º"""
        return cls(
            source_type=data.get('source_type', 'document'),
            title=data.get('title', ''),
            content=data.get('content', ''),
            url=data.get('url'),
            file_name=data.get('file_name'),
            relevance_score=data.get('relevance_score', 0.0)
        )


class KnowledgeService:
    """
    çŸ¥è¯†æœåŠ¡ï¼ˆä¸€æœŸç®€åŒ–ç‰ˆï¼‰
    
    åŠŸèƒ½ï¼š
    - å°†æ–‡æ¡£å†…å®¹è½¬æ¢ä¸ºçŸ¥è¯†æ¡ç›®
    - èåˆæ–‡æ¡£çŸ¥è¯†å’Œç½‘ç»œæœç´¢çŸ¥è¯†
    - ç®€å•å»é‡
    """
    
    def __init__(self, max_content_length: int = 8000):
        """
        åˆå§‹åŒ–çŸ¥è¯†æœåŠ¡
        
        Args:
            max_content_length: å•æ¡çŸ¥è¯†æœ€å¤§é•¿åº¦ï¼ˆè¶…è¿‡åˆ™æˆªæ–­ï¼‰
        """
        self.max_content_length = max_content_length
        logger.info(f"KnowledgeService åˆå§‹åŒ–å®Œæˆ, max_content_length={max_content_length}")
    
    def prepare_document_knowledge(
        self, 
        documents: List[Dict[str, Any]]
    ) -> List[KnowledgeItem]:
        """
        å°†æ–‡æ¡£è½¬æ¢ä¸ºçŸ¥è¯†æ¡ç›®ï¼ˆä¸€æœŸç®€åŒ–ï¼šæ•´ä¸ªæ–‡æ¡£ = 1 æ¡çŸ¥è¯†ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« {filename, markdown_content, ...}
        
        Returns:
            çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        """
        items = []
        
        for doc in documents:
            filename = doc.get('filename', '')
            markdown = doc.get('markdown_content', '')
            
            if not markdown:
                logger.warning(f"æ–‡æ¡£ {filename} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # æå–æ ‡é¢˜
            title = self._extract_title(markdown) or filename
            
            # æˆªæ–­å†…å®¹ï¼ˆä¸€æœŸç®€åŒ–ï¼‰
            content = self._truncate_content(markdown)
            
            item = KnowledgeItem(
                source_type='document',
                title=title,
                content=content,
                file_name=filename,
                relevance_score=1.0  # æ–‡æ¡£çŸ¥è¯†é»˜è®¤é«˜ç›¸å…³æ€§
            )
            items.append(item)
            
            logger.info(f"å‡†å¤‡æ–‡æ¡£çŸ¥è¯†: {title}, é•¿åº¦={len(content)}")
        
        return items
    
    def convert_search_results(
        self, 
        search_results: List[Dict[str, Any]]
    ) -> List[KnowledgeItem]:
        """
        å°†ç½‘ç»œæœç´¢ç»“æœè½¬æ¢ä¸ºçŸ¥è¯†æ¡ç›®
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
        
        Returns:
            çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        """
        items = []
        
        for result in search_results:
            title = result.get('title', '')
            content = result.get('content', '')
            url = result.get('url', '')
            
            if not content:
                continue
            
            item = KnowledgeItem(
                source_type='web_search',
                title=title,
                content=content,
                url=url,
                relevance_score=0.5  # ç½‘ç»œæœç´¢é»˜è®¤ä¸­ç­‰ç›¸å…³æ€§
            )
            items.append(item)
        
        return items
    
    def get_merged_knowledge(
        self,
        document_knowledge: List[KnowledgeItem],
        web_knowledge: List[KnowledgeItem],
        max_items: int = 20
    ) -> List[KnowledgeItem]:
        """
        èåˆæ–‡æ¡£çŸ¥è¯†å’Œç½‘ç»œæœç´¢çŸ¥è¯†
        
        ç­–ç•¥ï¼š
        1. æ–‡æ¡£çŸ¥è¯†ä¼˜å…ˆï¼ˆæœ€å¤š 10 æ¡ï¼‰
        2. ç½‘ç»œçŸ¥è¯†è¡¥å……
        3. ç®€å•å»é‡ï¼ˆåŸºäºæ ‡é¢˜/æ–‡ä»¶åï¼‰
        
        Args:
            document_knowledge: æ–‡æ¡£çŸ¥è¯†åˆ—è¡¨
            web_knowledge: ç½‘ç»œæœç´¢çŸ¥è¯†åˆ—è¡¨
            max_items: æœ€å¤§è¿”å›æ¡ç›®æ•°
        
        Returns:
            èåˆåçš„çŸ¥è¯†åˆ—è¡¨
        """
        result = []
        
        # 1. æ·»åŠ æ–‡æ¡£çŸ¥è¯†ï¼ˆæ•°é‡ä»é…ç½®è¯»å–ï¼‰
        max_doc_items = int(os.getenv('KNOWLEDGE_MAX_DOC_ITEMS', '10'))
        doc_count = min(len(document_knowledge), max_doc_items)
        result.extend(document_knowledge[:doc_count])
        logger.info(f"æ·»åŠ æ–‡æ¡£çŸ¥è¯†: {doc_count} æ¡")
        
        # 2. æ·»åŠ ç½‘ç»œçŸ¥è¯†ï¼ˆå»é‡ï¼‰
        web_added = 0
        for web_item in web_knowledge:
            if len(result) >= max_items:
                break
            
            if not self._is_duplicate_simple(web_item, result):
                result.append(web_item)
                web_added += 1
        
        logger.info(f"æ·»åŠ ç½‘ç»œçŸ¥è¯†: {web_added} æ¡")
        logger.info(f"èåˆå®Œæˆ: å…± {len(result)} æ¡çŸ¥è¯†")
        
        return result
    
    def summarize_for_prompt(
        self,
        knowledge_items: List[KnowledgeItem],
        max_total_length: int = 30000
    ) -> Dict[str, Any]:
        """
        å°†çŸ¥è¯†æ¡ç›®æ•´ç†ä¸º Prompt å¯ç”¨çš„æ ¼å¼
        
        Args:
            knowledge_items: çŸ¥è¯†æ¡ç›®åˆ—è¡¨
            max_total_length: æœ€å¤§æ€»é•¿åº¦
        
        Returns:
            {
                'background_knowledge': str,  # èƒŒæ™¯çŸ¥è¯†æ–‡æœ¬
                'document_references': list,  # æ–‡æ¡£æ¥æºåˆ—è¡¨
                'web_references': list        # ç½‘ç»œæ¥æºåˆ—è¡¨
            }
        """
        doc_refs = []
        web_refs = []
        knowledge_parts = []
        total_length = 0
        
        for item in knowledge_items:
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if total_length + len(item.content) > max_total_length:
                # æˆªæ–­
                remaining = max_total_length - total_length
                if remaining > 500:
                    truncated = item.content[:remaining] + "\n...(å†…å®¹å·²æˆªæ–­)"
                    knowledge_parts.append(f"### {item.title}\n\n{truncated}")
                break
            
            knowledge_parts.append(f"### {item.title}\n\n{item.content}")
            total_length += len(item.content)
            
            # æ”¶é›†å¼•ç”¨
            if item.source_type == 'document':
                doc_refs.append({
                    'title': item.title,
                    'file_name': item.file_name
                })
            else:
                web_refs.append({
                    'title': item.title,
                    'url': item.url
                })
        
        background_knowledge = "\n\n---\n\n".join(knowledge_parts)
        
        return {
            'background_knowledge': background_knowledge,
            'document_references': doc_refs,
            'web_references': web_refs
        }
    
    def _extract_title(self, markdown: str) -> Optional[str]:
        """ä» Markdown ä¸­æå–æ ‡é¢˜"""
        # å°è¯•åŒ¹é… # æ ‡é¢˜
        match = re.search(r'^#\s+(.+)$', markdown, re.MULTILINE)
        if match:
            return match.group(1).strip()
        
        # å°è¯•åŒ¹é…ç¬¬ä¸€è¡Œéç©ºå†…å®¹
        lines = markdown.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                # æˆªå–å‰ 50 ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
                return line[:50] + ('...' if len(line) > 50 else '')
        
        return None
    
    def _truncate_content(self, content: str) -> str:
        """æˆªæ–­å†…å®¹åˆ°æœ€å¤§é•¿åº¦"""
        if len(content) <= self.max_content_length:
            return content
        
        truncated = content[:self.max_content_length]
        return truncated + f"\n\n...(å†…å®¹å·²æˆªæ–­ï¼ŒåŸæ–‡å…± {len(content)} å­—ç¬¦)"
    
    def _is_duplicate_simple(
        self, 
        item: KnowledgeItem, 
        existing: List[KnowledgeItem]
    ) -> bool:
        """
        ç®€å•å»é‡ï¼ˆä¸€æœŸï¼‰ï¼šåŸºäºæ ‡é¢˜/æ–‡ä»¶å
        
        Args:
            item: å¾…æ£€æŸ¥çš„çŸ¥è¯†æ¡ç›®
            existing: å·²æœ‰çš„çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        
        Returns:
            æ˜¯å¦é‡å¤
        """
        for e in existing:
            # åŒä¸€æ–‡ä»¶
            if item.file_name and item.file_name == e.file_name:
                return True
            # æ ‡é¢˜ç›¸åŒ
            if item.title and item.title == e.title:
                return True
        return False
    
    # ========== äºŒæœŸæ–°å¢ï¼šä¸¤çº§ç»“æ„æ£€ç´¢ ==========
    
    def prepare_chunked_knowledge(
        self,
        documents: List[Dict[str, Any]],
        chunks: List[Dict[str, Any]],
        images: List[Dict[str, Any]] = None
    ) -> List[KnowledgeItem]:
        """
        å‡†å¤‡åˆ†å—çŸ¥è¯†ï¼ˆäºŒæœŸï¼‰
        
        ä¸¤çº§ç»“æ„ï¼š
        1. æ–‡æ¡£çº§ï¼šæ‘˜è¦ + å…ƒä¿¡æ¯
        2. åˆ†å—çº§ï¼šå…·ä½“å†…å®¹ç‰‡æ®µ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å« {filename, summary, ...}
            chunks: åˆ†å—åˆ—è¡¨ï¼ŒåŒ…å« {document_id, title, content, ...}
            images: å›¾ç‰‡åˆ—è¡¨ï¼ŒåŒ…å« {document_id, caption, ...}
        
        Returns:
            çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        """
        items = []
        images = images or []
        
        # æŒ‰æ–‡æ¡£ ID åˆ†ç»„
        doc_map = {doc.get('id'): doc for doc in documents}
        chunks_by_doc = {}
        images_by_doc = {}
        
        for chunk in chunks:
            doc_id = chunk.get('document_id')
            if doc_id not in chunks_by_doc:
                chunks_by_doc[doc_id] = []
            chunks_by_doc[doc_id].append(chunk)
        
        for img in images:
            doc_id = img.get('document_id')
            if doc_id not in images_by_doc:
                images_by_doc[doc_id] = []
            images_by_doc[doc_id].append(img)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºçŸ¥è¯†æ¡ç›®
        for doc_id, doc in doc_map.items():
            filename = doc.get('filename', '')
            summary = doc.get('summary', '')
            doc_chunks = chunks_by_doc.get(doc_id, [])
            doc_images = images_by_doc.get(doc_id, [])
            
            # 1. æ–‡æ¡£çº§æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if summary:
                items.append(KnowledgeItem(
                    source_type='document',
                    title=f"{filename} - æ‘˜è¦",
                    content=summary,
                    file_name=filename,
                    relevance_score=1.0
                ))
            
            # 2. åˆ†å—çº§å†…å®¹
            for chunk in doc_chunks:
                chunk_title = chunk.get('title', '')
                chunk_content = chunk.get('content', '')
                
                if not chunk_content:
                    continue
                
                # æˆªæ–­è¿‡é•¿å†…å®¹
                content = self._truncate_content(chunk_content)
                
                items.append(KnowledgeItem(
                    source_type='document',
                    title=f"{filename} - {chunk_title}" if chunk_title else filename,
                    content=content,
                    file_name=filename,
                    relevance_score=0.9
                ))
            
            # 3. å›¾ç‰‡æ‘˜è¦ï¼ˆä½œä¸ºè¡¥å……çŸ¥è¯†ï¼‰
            if doc_images:
                image_captions = []
                for img in doc_images:
                    caption = img.get('caption', '')
                    if caption:
                        page_num = img.get('page_num', 0)
                        image_captions.append(f"- ç¬¬{page_num}é¡µå›¾ç‰‡: {caption}")
                
                if image_captions:
                    items.append(KnowledgeItem(
                        source_type='document',
                        title=f"{filename} - å›¾ç‰‡å†…å®¹",
                        content="\n".join(image_captions),
                        file_name=filename,
                        relevance_score=0.7
                    ))
        
        logger.info(f"å‡†å¤‡åˆ†å—çŸ¥è¯†: {len(items)} æ¡ (æ¥è‡ª {len(documents)} ä¸ªæ–‡æ¡£)")
        return items
    
    def get_merged_knowledge_v2(
        self,
        documents: List[Dict[str, Any]],
        chunks: List[Dict[str, Any]],
        images: List[Dict[str, Any]],
        web_knowledge: List[KnowledgeItem],
        max_items: int = 30
    ) -> List[KnowledgeItem]:
        """
        èåˆåˆ†å—çŸ¥è¯†å’Œç½‘ç»œæœç´¢çŸ¥è¯†ï¼ˆäºŒæœŸï¼‰
        
        ç­–ç•¥ï¼š
        1. æ–‡æ¡£æ‘˜è¦ä¼˜å…ˆ
        2. ç›¸å…³åˆ†å—è¡¥å……
        3. ç½‘ç»œçŸ¥è¯†å¡«å……
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            chunks: åˆ†å—åˆ—è¡¨
            images: å›¾ç‰‡åˆ—è¡¨
            web_knowledge: ç½‘ç»œæœç´¢çŸ¥è¯†
            max_items: æœ€å¤§è¿”å›æ¡ç›®æ•°
        
        Returns:
            èåˆåçš„çŸ¥è¯†åˆ—è¡¨
        """
        # å‡†å¤‡åˆ†å—çŸ¥è¯†
        doc_knowledge = self.prepare_chunked_knowledge(documents, chunks, images)
        
        result = []
        max_doc_items = int(os.getenv('KNOWLEDGE_MAX_DOC_ITEMS', '10'))
        
        # 1. æ·»åŠ æ–‡æ¡£çŸ¥è¯†ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
        doc_knowledge.sort(key=lambda x: x.relevance_score, reverse=True)
        doc_count = min(len(doc_knowledge), max_doc_items)
        result.extend(doc_knowledge[:doc_count])
        logger.info(f"æ·»åŠ æ–‡æ¡£çŸ¥è¯†: {doc_count} æ¡")
        
        # 2. æ·»åŠ ç½‘ç»œçŸ¥è¯†ï¼ˆå»é‡ï¼‰
        web_added = 0
        for web_item in web_knowledge:
            if len(result) >= max_items:
                break
            
            if not self._is_duplicate_simple(web_item, result):
                result.append(web_item)
                web_added += 1
        
        logger.info(f"æ·»åŠ ç½‘ç»œçŸ¥è¯†: {web_added} æ¡")
        logger.info(f"èåˆå®Œæˆ (v2): å…± {len(result)} æ¡çŸ¥è¯†")
        
        return result
    
    def summarize_for_prompt_v2(
        self,
        knowledge_items: List[KnowledgeItem],
        max_total_length: int = 30000
    ) -> Dict[str, Any]:
        """
        å°†çŸ¥è¯†æ¡ç›®æ•´ç†ä¸º Prompt å¯ç”¨çš„æ ¼å¼ï¼ˆäºŒæœŸå¢å¼ºï¼‰
        
        å¢å¼ºï¼šæŒ‰æ–‡æ¡£åˆ†ç»„å±•ç¤º
        
        Args:
            knowledge_items: çŸ¥è¯†æ¡ç›®åˆ—è¡¨
            max_total_length: æœ€å¤§æ€»é•¿åº¦
        
        Returns:
            {
                'background_knowledge': str,
                'document_references': list,
                'web_references': list,
                'knowledge_stats': dict
            }
        """
        doc_refs = []
        web_refs = []
        
        # æŒ‰æ¥æºåˆ†ç»„
        doc_items = [i for i in knowledge_items if i.source_type == 'document']
        web_items = [i for i in knowledge_items if i.source_type == 'web_search']
        
        knowledge_parts = []
        total_length = 0
        
        # æ–‡æ¡£çŸ¥è¯†
        if doc_items:
            knowledge_parts.append("## ğŸ“š æ–‡æ¡£çŸ¥è¯†\n")
            seen_files = set()
            
            for item in doc_items:
                if total_length + len(item.content) > max_total_length:
                    remaining = max_total_length - total_length
                    if remaining > 500:
                        truncated = item.content[:remaining] + "\n...(å†…å®¹å·²æˆªæ–­)"
                        knowledge_parts.append(f"### {item.title}\n\n{truncated}")
                    break
                
                knowledge_parts.append(f"### {item.title}\n\n{item.content}")
                total_length += len(item.content)
                
                if item.file_name and item.file_name not in seen_files:
                    doc_refs.append({
                        'title': item.title.split(' - ')[0] if ' - ' in item.title else item.title,
                        'file_name': item.file_name
                    })
                    seen_files.add(item.file_name)
        
        # ç½‘ç»œçŸ¥è¯†
        if web_items and total_length < max_total_length:
            knowledge_parts.append("\n## ğŸŒ ç½‘ç»œçŸ¥è¯†\n")
            
            for item in web_items:
                if total_length + len(item.content) > max_total_length:
                    break
                
                knowledge_parts.append(f"### {item.title}\n\n{item.content}")
                total_length += len(item.content)
                
                web_refs.append({
                    'title': item.title,
                    'url': item.url
                })
        
        background_knowledge = "\n\n".join(knowledge_parts)
        
        return {
            'background_knowledge': background_knowledge,
            'document_references': doc_refs,
            'web_references': web_refs,
            'knowledge_stats': {
                'doc_items': len(doc_items),
                'web_items': len(web_items),
                'total_length': total_length
            }
        }


# å…¨å±€å•ä¾‹
_knowledge_service: Optional[KnowledgeService] = None


def get_knowledge_service() -> KnowledgeService:
    """è·å–çŸ¥è¯†æœåŠ¡å•ä¾‹"""
    global _knowledge_service
    if _knowledge_service is None:
        _knowledge_service = KnowledgeService()
    return _knowledge_service


def init_knowledge_service(max_content_length: int = 8000) -> KnowledgeService:
    """åˆå§‹åŒ–çŸ¥è¯†æœåŠ¡"""
    global _knowledge_service
    _knowledge_service = KnowledgeService(max_content_length=max_content_length)
    return _knowledge_service
